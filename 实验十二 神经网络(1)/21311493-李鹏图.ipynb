{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59a56ee",
   "metadata": {},
   "source": [
    "**<font color = black size=6>实验十二：神经网络</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "62f100c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "import torch.utils.data as Data\n",
    "import os  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c03386",
   "metadata": {},
   "source": [
    "本实验使用Pytorch框架搭建神经网络，其他类似的框架还有TensorFlow。若同学对TensorFlow框架更为熟悉，可使用TensorFlow完成本次实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebaf0fe",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第一部分:PyTorch介绍</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a394db6",
   "metadata": {},
   "source": [
    "这里介绍一小部分PyTorch常用的库和函数，更多需求可参阅[PyTorch官方教程](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)以及[PyTorch官方文档](https://pytorch.org/docs/stable/index.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "6fae149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # 输出当前版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392780bb",
   "metadata": {},
   "source": [
    "**<font color = green size=3>1.Tensor</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904623e4",
   "metadata": {},
   "source": [
    "Tensor与NumPy中的ndarray很相似，但Tensor可以利用GPU来加速计算（本次实验中暂不涉及这部分内容）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23642996",
   "metadata": {},
   "source": [
    "1.1. Tensor的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "38cdf3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 4.3750, 0.0000],\n",
      "        [0.0000, 0.0000, 4.6602]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[0.4984, 0.0829, 0.0763, 0.0897],\n",
      "        [0.9556, 0.1520, 0.0471, 0.2106],\n",
      "        [0.1513, 0.7917, 0.5539, 0.3053]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个未初始化的Tensor\n",
    "x = torch.empty(2, 3)\n",
    "print(x)\n",
    "\n",
    "# 从一个列表创建Tensor\n",
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "\n",
    "# 创建一个随机Tensor\n",
    "x = torch.rand([3, 4])\n",
    "print(x)\n",
    "\n",
    "# 创建一个全零Tensor\n",
    "x = torch.zeros([2, 3])\n",
    "print(x)\n",
    "\n",
    "# 创建一个全一Tensor\n",
    "x = torch.ones([2, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f354fd4",
   "metadata": {},
   "source": [
    "1.2. Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "8070a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7]])\n",
      "tensor([[-5, -3, -1],\n",
      "        [ 1,  3,  5]])\n",
      "tensor([[ 6, 10, 12],\n",
      "        [12, 10,  6]])\n",
      "tensor([[ 6, 10, 12],\n",
      "        [12, 10,  6]])\n",
      "tensor([[28, 10],\n",
      "        [73, 28]])\n",
      "tensor([[28, 10],\n",
      "        [73, 28]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [6, 5, 4],\n",
      "        [3, 2, 1]])\n",
      "tensor([[1, 2, 3, 6, 5, 4],\n",
      "        [4, 5, 6, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 加减法\n",
    "x = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]])\n",
    "y = torch.tensor([[6,5,4],\n",
    "                  [3,2,1]])\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "\n",
    "# 对应位置相乘\n",
    "print(x * y)\n",
    "print(x.mul(y))\n",
    "\n",
    "# 矩阵乘法\n",
    "print(x.matmul(y.T))\n",
    "print(x @ y.T)\n",
    "\n",
    "# reshape\n",
    "print(x.reshape(3, 2))\n",
    "\n",
    "# 拼接\n",
    "print(torch.cat([x,y], dim=0)) # 纵向拼接\n",
    "print(torch.cat([x,y], dim=1)) # 横向拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95961cee",
   "metadata": {},
   "source": [
    "1.3. Tensor与ndarray的相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d1269212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "\n",
    "# 从Tensor转换到ndarray\n",
    "y = x.numpy()\n",
    "print(y)\n",
    "\n",
    "# Tensor与ndarray是共享空间的\n",
    "x[:]=0\n",
    "print(y)\n",
    "\n",
    "# 从ndarray到Tensor\n",
    "z = torch.from_numpy(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c36a7",
   "metadata": {},
   "source": [
    "**<font color = green size=3>2.梯度计算</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3f1a6",
   "metadata": {},
   "source": [
    "2.1 梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9113eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.]])\n",
      "None\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#定义变量\n",
    "a = torch.tensor([[1., 2.]], requires_grad=True)\n",
    "b = torch.tensor([[3.], [4.]])\n",
    "c = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "#计算输出\n",
    "z = a @ b + c\n",
    "\n",
    "#自动计算梯度\n",
    "z.backward()\n",
    "\n",
    "#输出叶子节点的梯度\n",
    "print(a.grad) #z对a的梯度\n",
    "print(b.grad) #由于b默认requires_grad为false，因此无法计算梯度，输出为None\n",
    "print(c.grad) #z对c的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a046cd",
   "metadata": {},
   "source": [
    "2.2 梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "04cda9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度（a.grad）: tensor([0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966,\n",
      "        0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966,\n",
      "        0.1966, 0.1966])\n",
      "求梯度后的结果（x.grad）: tensor(4.)\n",
      "求梯度后的结果（x.grad）: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#支持多种运算求梯度，如torch.mean(),torch.sum()等\n",
    "a = torch.ones(20, requires_grad=True)\n",
    "z = torch.sum(torch.sigmoid(a))\n",
    "z.backward()\n",
    "print(\"梯度（a.grad）:\", a.grad)\n",
    "\n",
    "\n",
    "#多次求梯度时梯度会累加，可使用tensor.grad.zero_()进行手动清零\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(\"求梯度后的结果（x.grad）:\", x.grad)\n",
    "\n",
    "z = x + 3\n",
    "x.grad.zero_()  #可以将这句进行手动清零的代码注释掉后查看输出结果，来看到梯度清零的作用\n",
    "z.backward()\n",
    "print(\"求梯度后的结果（x.grad）:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d22808",
   "metadata": {},
   "source": [
    "**<font color = green size=3>3. 神经网络</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2064274",
   "metadata": {},
   "source": [
    "3.1 神经网络的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "207b6433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络模型，继承自nn.Module\n",
    "class Net(nn.Module):\n",
    "    #输入层的维度为 input_dim\n",
    "    #隐藏层的维度为 hidden_dim\n",
    "    #输出层的维度为 output_dim\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        #激活函数relu，用于在全连接层之间加入非线性变换\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out1 = self.relu(out)\n",
    "        out2 = self.fc2(out1)\n",
    "        return out1,out2\n",
    "\n",
    "\n",
    "# 创建神经网络模型实例并输出\n",
    "net = Net(10,5,1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efbf82",
   "metadata": {},
   "source": [
    "3.2 神经网络参数查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "6c2a943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([5, 10]), torch.Size([5]), torch.Size([1, 5]), torch.Size([1])]\n",
      "Parameters: [Parameter containing:\n",
      "tensor([[ 0.2254,  0.1326, -0.2645, -0.2574,  0.1168, -0.0413,  0.0550,  0.0943,\n",
      "         -0.2911, -0.0972],\n",
      "        [-0.1666,  0.2227,  0.1626, -0.1171,  0.1549,  0.1910, -0.1510, -0.2714,\n",
      "         -0.0763, -0.2110],\n",
      "        [-0.2266,  0.1093, -0.3015,  0.0005,  0.0224, -0.0306, -0.2671,  0.2207,\n",
      "         -0.1438,  0.0893],\n",
      "        [-0.0893,  0.0357,  0.2323,  0.1205,  0.2425, -0.2000, -0.0400, -0.0605,\n",
      "         -0.0683,  0.1537],\n",
      "        [-0.2201, -0.2474, -0.0028, -0.0483,  0.0982,  0.2857, -0.0210, -0.0109,\n",
      "          0.1122,  0.1370]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2239,  0.0632, -0.2579, -0.0268,  0.0507], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1764, -0.3354, -0.0063, -0.1894, -0.2073]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1283], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 该神经网络中可学习的参数可以通过net.parameters()访问\n",
    "params = list(net.parameters())\n",
    "print([params[i].size() for i in range(len(params))])  \n",
    "print(\"Parameters:\",params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5c67",
   "metadata": {},
   "source": [
    "3.3 神经网络前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b733263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of first layer: tensor([[0.0000, 0.0000, 0.0000, 0.2999, 0.1334]], grad_fn=<ReluBackward0>)\n",
      "Output of second layer: tensor([[0.0439]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "#输入维度为10，生成数据\n",
    "input=torch.ones([1,10])\n",
    "input=input.float()\n",
    "\n",
    "# 进行一次forward()前向传播\n",
    "output1, output2  = net(input) \n",
    "\n",
    "# 前向传播并输出每一层的输出值\n",
    "print(\"Output of first layer:\", output1)\n",
    "print(\"Output of second layer:\", output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89909999",
   "metadata": {},
   "source": [
    "3.4 神经网络反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "8c8f10b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of first layer:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294,\n",
      "         -0.2294, -0.2294],\n",
      "        [-0.2512, -0.2512, -0.2512, -0.2512, -0.2512, -0.2512, -0.2512, -0.2512,\n",
      "         -0.2512, -0.2512]])\n",
      "tensor([ 0.0000,  0.0000,  0.0000, -0.2294, -0.2512])\n",
      "Gradients of second layer:\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.3633, 0.1616]])\n",
      "tensor([1.2115])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn(1, 1)\n",
    "loss = loss_fn(output2, target)\n",
    "\n",
    "# 反向传播并输出每一层的梯度\n",
    "net.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradients of first layer:\")\n",
    "print(net.fc1.weight.grad)\n",
    "print(net.fc1.bias.grad)\n",
    "\n",
    "print(\"Gradients of second layer:\")\n",
    "print(net.fc2.weight.grad)\n",
    "print(net.fc2.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9fb6e",
   "metadata": {},
   "source": [
    "3.5 训练神经网络的全过程例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4126d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1804]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9719], requires_grad=True)]\n",
      "Epoch [1/100], Loss: 9.32189655303955 . Gradient: tensor([[-13.0954]])\n",
      "Epoch [2/100], Loss: 7.433237552642822 . Gradient: tensor([[-11.6598]])\n",
      "Epoch [3/100], Loss: 5.940042018890381 . Gradient: tensor([[-10.3834]])\n",
      "Epoch [4/100], Loss: 4.759441375732422 . Gradient: tensor([[-9.2485]])\n",
      "Epoch [5/100], Loss: 3.8259308338165283 . Gradient: tensor([[-8.2395]])\n",
      "Epoch [6/100], Loss: 3.0877339839935303 . Gradient: tensor([[-7.3424]])\n",
      "Epoch [7/100], Loss: 2.5039241313934326 . Gradient: tensor([[-6.5447]])\n",
      "Epoch [8/100], Loss: 2.0421500205993652 . Gradient: tensor([[-5.8355]])\n",
      "Epoch [9/100], Loss: 1.6768399477005005 . Gradient: tensor([[-5.2050]])\n",
      "Epoch [10/100], Loss: 1.387781023979187 . Gradient: tensor([[-4.6443]])\n",
      "Epoch [11/100], Loss: 1.1589962244033813 . Gradient: tensor([[-4.1458]])\n",
      "Epoch [12/100], Loss: 0.977856457233429 . Gradient: tensor([[-3.7025]])\n",
      "Epoch [13/100], Loss: 0.8343785405158997 . Gradient: tensor([[-3.3084]])\n",
      "Epoch [14/100], Loss: 0.7206719517707825 . Gradient: tensor([[-2.9580]])\n",
      "Epoch [15/100], Loss: 0.6304991245269775 . Gradient: tensor([[-2.6464]])\n",
      "Epoch [16/100], Loss: 0.5589305758476257 . Gradient: tensor([[-2.3693]])\n",
      "Epoch [17/100], Loss: 0.5020686984062195 . Gradient: tensor([[-2.1229]])\n",
      "Epoch [18/100], Loss: 0.4568326771259308 . Gradient: tensor([[-1.9038]])\n",
      "Epoch [19/100], Loss: 0.42078813910484314 . Gradient: tensor([[-1.7090]])\n",
      "Epoch [20/100], Loss: 0.3920100927352905 . Gradient: tensor([[-1.5357]])\n",
      "Epoch [21/100], Loss: 0.3689776360988617 . Gradient: tensor([[-1.3816]])\n",
      "Epoch [22/100], Loss: 0.35048773884773254 . Gradient: tensor([[-1.2446]])\n",
      "Epoch [23/100], Loss: 0.3355896770954132 . Gradient: tensor([[-1.1228]])\n",
      "Epoch [24/100], Loss: 0.3235325813293457 . Gradient: tensor([[-1.0144]])\n",
      "Epoch [25/100], Loss: 0.31372228264808655 . Gradient: tensor([[-0.9180]])\n",
      "Epoch [26/100], Loss: 0.3056894838809967 . Gradient: tensor([[-0.8322]])\n",
      "Epoch [27/100], Loss: 0.2990627884864807 . Gradient: tensor([[-0.7560]])\n",
      "Epoch [28/100], Loss: 0.2935487926006317 . Gradient: tensor([[-0.6881]])\n",
      "Epoch [29/100], Loss: 0.2889159023761749 . Gradient: tensor([[-0.6277]])\n",
      "Epoch [30/100], Loss: 0.28498080372810364 . Gradient: tensor([[-0.5740]])\n",
      "Epoch [31/100], Loss: 0.28159835934638977 . Gradient: tensor([[-0.5263]])\n",
      "Epoch [32/100], Loss: 0.2786541283130646 . Gradient: tensor([[-0.4837]])\n",
      "Epoch [33/100], Loss: 0.27605772018432617 . Gradient: tensor([[-0.4459]])\n",
      "Epoch [34/100], Loss: 0.2737375795841217 . Gradient: tensor([[-0.4122]])\n",
      "Epoch [35/100], Loss: 0.27163684368133545 . Gradient: tensor([[-0.3822]])\n",
      "Epoch [36/100], Loss: 0.2697109878063202 . Gradient: tensor([[-0.3555]])\n",
      "Epoch [37/100], Loss: 0.2679245173931122 . Gradient: tensor([[-0.3318]])\n",
      "Epoch [38/100], Loss: 0.26624956727027893 . Gradient: tensor([[-0.3106]])\n",
      "Epoch [39/100], Loss: 0.2646641433238983 . Gradient: tensor([[-0.2917]])\n",
      "Epoch [40/100], Loss: 0.2631504535675049 . Gradient: tensor([[-0.2749]])\n",
      "Epoch [41/100], Loss: 0.2616952955722809 . Gradient: tensor([[-0.2599]])\n",
      "Epoch [42/100], Loss: 0.26028698682785034 . Gradient: tensor([[-0.2466]])\n",
      "Epoch [43/100], Loss: 0.2589171230792999 . Gradient: tensor([[-0.2347]])\n",
      "Epoch [44/100], Loss: 0.2575792968273163 . Gradient: tensor([[-0.2240]])\n",
      "Epoch [45/100], Loss: 0.25626733899116516 . Gradient: tensor([[-0.2146]])\n",
      "Epoch [46/100], Loss: 0.25497764348983765 . Gradient: tensor([[-0.2061]])\n",
      "Epoch [47/100], Loss: 0.2537064254283905 . Gradient: tensor([[-0.1985]])\n",
      "Epoch [48/100], Loss: 0.252451092004776 . Gradient: tensor([[-0.1918]])\n",
      "Epoch [49/100], Loss: 0.2512095868587494 . Gradient: tensor([[-0.1857]])\n",
      "Epoch [50/100], Loss: 0.2499801516532898 . Gradient: tensor([[-0.1803]])\n",
      "Epoch [51/100], Loss: 0.24876146018505096 . Gradient: tensor([[-0.1754]])\n",
      "Epoch [52/100], Loss: 0.24755239486694336 . Gradient: tensor([[-0.1711]])\n",
      "Epoch [53/100], Loss: 0.24635232985019684 . Gradient: tensor([[-0.1672]])\n",
      "Epoch [54/100], Loss: 0.2451602965593338 . Gradient: tensor([[-0.1637]])\n",
      "Epoch [55/100], Loss: 0.24397589266300201 . Gradient: tensor([[-0.1605]])\n",
      "Epoch [56/100], Loss: 0.24279862642288208 . Gradient: tensor([[-0.1577]])\n",
      "Epoch [57/100], Loss: 0.24162833392620087 . Gradient: tensor([[-0.1551]])\n",
      "Epoch [58/100], Loss: 0.2404644638299942 . Gradient: tensor([[-0.1528]])\n",
      "Epoch [59/100], Loss: 0.2393067628145218 . Gradient: tensor([[-0.1507]])\n",
      "Epoch [60/100], Loss: 0.23815546929836273 . Gradient: tensor([[-0.1488]])\n",
      "Epoch [61/100], Loss: 0.2370101660490036 . Gradient: tensor([[-0.1471]])\n",
      "Epoch [62/100], Loss: 0.23587055504322052 . Gradient: tensor([[-0.1455]])\n",
      "Epoch [63/100], Loss: 0.23473693430423737 . Gradient: tensor([[-0.1441]])\n",
      "Epoch [64/100], Loss: 0.23360873758792877 . Gradient: tensor([[-0.1428]])\n",
      "Epoch [65/100], Loss: 0.23248611390590668 . Gradient: tensor([[-0.1416]])\n",
      "Epoch [66/100], Loss: 0.2313692420721054 . Gradient: tensor([[-0.1405]])\n",
      "Epoch [67/100], Loss: 0.23025773465633392 . Gradient: tensor([[-0.1395]])\n",
      "Epoch [68/100], Loss: 0.22915184497833252 . Gradient: tensor([[-0.1386]])\n",
      "Epoch [69/100], Loss: 0.22805123031139374 . Gradient: tensor([[-0.1377]])\n",
      "Epoch [70/100], Loss: 0.226955845952034 . Gradient: tensor([[-0.1369]])\n",
      "Epoch [71/100], Loss: 0.2258657068014145 . Gradient: tensor([[-0.1361]])\n",
      "Epoch [72/100], Loss: 0.22478096187114716 . Gradient: tensor([[-0.1354]])\n",
      "Epoch [73/100], Loss: 0.22370141744613647 . Gradient: tensor([[-0.1348]])\n",
      "Epoch [74/100], Loss: 0.2226271629333496 . Gradient: tensor([[-0.1342]])\n",
      "Epoch [75/100], Loss: 0.22155797481536865 . Gradient: tensor([[-0.1336]])\n",
      "Epoch [76/100], Loss: 0.22049391269683838 . Gradient: tensor([[-0.1330]])\n",
      "Epoch [77/100], Loss: 0.2194352000951767 . Gradient: tensor([[-0.1325]])\n",
      "Epoch [78/100], Loss: 0.21838124096393585 . Gradient: tensor([[-0.1320]])\n",
      "Epoch [79/100], Loss: 0.21733267605304718 . Gradient: tensor([[-0.1315]])\n",
      "Epoch [80/100], Loss: 0.21628902852535248 . Gradient: tensor([[-0.1310]])\n",
      "Epoch [81/100], Loss: 0.2152504175901413 . Gradient: tensor([[-0.1306]])\n",
      "Epoch [82/100], Loss: 0.2142166942358017 . Gradient: tensor([[-0.1302]])\n",
      "Epoch [83/100], Loss: 0.21318791806697845 . Gradient: tensor([[-0.1298]])\n",
      "Epoch [84/100], Loss: 0.21216420829296112 . Gradient: tensor([[-0.1294]])\n",
      "Epoch [85/100], Loss: 0.2111453264951706 . Gradient: tensor([[-0.1290]])\n",
      "Epoch [86/100], Loss: 0.21013160049915314 . Gradient: tensor([[-0.1286]])\n",
      "Epoch [87/100], Loss: 0.20912231504917145 . Gradient: tensor([[-0.1282]])\n",
      "Epoch [88/100], Loss: 0.20811831951141357 . Gradient: tensor([[-0.1278]])\n",
      "Epoch [89/100], Loss: 0.2071189135313034 . Gradient: tensor([[-0.1275]])\n",
      "Epoch [90/100], Loss: 0.2061242014169693 . Gradient: tensor([[-0.1271]])\n",
      "Epoch [91/100], Loss: 0.20513443648815155 . Gradient: tensor([[-0.1268]])\n",
      "Epoch [92/100], Loss: 0.2041492909193039 . Gradient: tensor([[-0.1264]])\n",
      "Epoch [93/100], Loss: 0.20316894352436066 . Gradient: tensor([[-0.1261]])\n",
      "Epoch [94/100], Loss: 0.20219333469867706 . Gradient: tensor([[-0.1258]])\n",
      "Epoch [95/100], Loss: 0.20122243463993073 . Gradient: tensor([[-0.1254]])\n",
      "Epoch [96/100], Loss: 0.20025599002838135 . Gradient: tensor([[-0.1251]])\n",
      "Epoch [97/100], Loss: 0.19929444789886475 . Gradient: tensor([[-0.1248]])\n",
      "Epoch [98/100], Loss: 0.19833742082118988 . Gradient: tensor([[-0.1245]])\n",
      "Epoch [99/100], Loss: 0.1973849982023239 . Gradient: tensor([[-0.1242]])\n",
      "Epoch [100/100], Loss: 0.1964370608329773 . Gradient: tensor([[-0.1239]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个简单的线性回归模型\n",
    "model = nn.Linear(1, 1)\n",
    "print(list(model.parameters()))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "Epoch=100\n",
    "\n",
    "#生成数据\n",
    "inputs = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "labels = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "# 模拟训练过程\n",
    "for epoch in range(Epoch):\n",
    "    # 模拟输入数据和标签\n",
    "    \n",
    "    # 前向传播\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    #梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    # 打印梯度值\n",
    "    print('Epoch [{}/{}], Loss: {}'.format(epoch+1,Epoch, loss),'. Gradient: {}'.format(model.weight.grad))\n",
    "    #print('Gradient: {}'.format(model.weight.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d39b00",
   "metadata": {},
   "source": [
    "3.5 神经网络参数更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f54bd7",
   "metadata": {},
   "source": [
    "1) 用梯度下降法(手动)更新net中的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d7187507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for f in net.parameters():\n",
    "    #f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee49df5",
   "metadata": {},
   "source": [
    "2) 用PyTorch的优化器来更新net中的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "06d237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择优化器\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 建立循环:\n",
    "#optimizer.zero_grad()             # 梯度清零\n",
    "#output = net(input)               # 前向传播\n",
    "#loss = criterion(output, target)  # 计算误差\n",
    "#loss.backward()                   # 后向传播\n",
    "#optimizer.step()                  # 参数更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be2d1f",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第二部分:实验内容</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5214e5",
   "metadata": {},
   "source": [
    "[Red Wine Quality](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009)是一个关于红酒品质的数据集，总共有1599个样本，每个样本包含11个(都是连续的)特征以及1个标签，每个标签的取值是连续的。本次实验已经按照8：2的比例划分成了训练数据集'wine_train.csv'以及测试数据集'wine_test.csv'，且每个数据集都已经做了归一化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9ccac",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">1) 读入训练数据集'wine_train.csv'与测试数据集'wine_test.csv'。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "c759660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 12)\n",
      "(320, 12)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "train_df = pd.read_csv('./wine_train.csv')\n",
    "test_df = pd.read_csv('./wine_test.csv')\n",
    "test_df = test_df.to_numpy()\n",
    "train_df = train_df.to_numpy()\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77619fd",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">2) 利用线性层和激活函数搭建一个神经网络，要求输入和输出维度与数据集维度一致，而神经网络深度、隐藏层大小、激活函数种类等超参数自行调整。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "26359bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=11, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim,device):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim,bias=True,device=device,dtype=torch.float32)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim,bias=True,device=device,dtype=torch.float32)\n",
    "        self.fc3 = nn.Linear(hidden_dim,hidden_dim,bias=True,device=device,dtype=torch.float32)\n",
    "        self.fc4 = nn.Linear(hidden_dim,output_dim,bias=True,device=device,dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_dim = 11\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "device = 'cpu'\n",
    "\n",
    "# 创建网络对象\n",
    "net = Net(input_dim,output_dim,hidden_dim,device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b706eea",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">3) 用PyTorch的优化器(随机梯度下降)来进行模型参数更新，记下每轮迭代中的训练损失和测试损失。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "cad2f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ff45e",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">4) 画出训练损失和测试损失关于迭代轮数的折线图。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3ba3071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31836286187171936, 0.19884885847568512, 0.12938982248306274, 0.0871828943490982, 0.061329834163188934, 0.045734159648418427, 0.03656827658414841, 0.0313008576631546, 0.028361981734633446, 0.026732467114925385, 0.025820089504122734, 0.02530776895582676, 0.024999571964144707, 0.02481023781001568, 0.024681804701685905, 0.024580786004662514, 0.02449830248951912, 0.024422181770205498, 0.024349525570869446, 0.02427956461906433, 0.024210484698414803, 0.024142030626535416, 0.02407403476536274, 0.02400677651166916, 0.0239403173327446, 0.02387474663555622, 0.023809533566236496, 0.023744231089949608, 0.023679964244365692, 0.023616323247551918, 0.02355322800576687, 0.02349131926894188, 0.02343008853495121, 0.023369718343019485, 0.02331058494746685, 0.02325195074081421, 0.023194024339318275, 0.023136919364333153, 0.023080449551343918, 0.023024514317512512, 0.022969171404838562, 0.022914867848157883, 0.02286078967154026, 0.02280721627175808, 0.02275405265390873, 0.022701464593410492, 0.02264915220439434, 0.02259739115834236, 0.022546062245965004, 0.022495243698358536, 0.022444704547524452, 0.022394822910428047, 0.022345438599586487, 0.022296225652098656, 0.02224711701273918, 0.02219836600124836, 0.022150026634335518, 0.02210201695561409, 0.022054273635149002, 0.022007107734680176, 0.02196003869175911, 0.021913187578320503, 0.021866757422685623, 0.02182070165872574]\n",
      "[0.2058367282152176, 0.1343121975660324, 0.09071819484233856, 0.06392501294612885, 0.047720856964588165, 0.0381607785820961, 0.032635536044836044, 0.02953031100332737, 0.027791772037744522, 0.026804964989423752, 0.026240918785333633, 0.0258941650390625, 0.025675764307379723, 0.02552478387951851, 0.02540486492216587, 0.025308173149824142, 0.02521914802491665, 0.0251346193253994, 0.02505548857152462, 0.02497742511332035, 0.024900633841753006, 0.024825429543852806, 0.02475143037736416, 0.024676313623785973, 0.024601852521300316, 0.024531209841370583, 0.02446097508072853, 0.024392111226916313, 0.024321507662534714, 0.024253161624073982, 0.024186071008443832, 0.024117017164826393, 0.02404952421784401, 0.02398298680782318, 0.02391980215907097, 0.02385854348540306, 0.02379421889781952, 0.023734478279948235, 0.02367134764790535, 0.023611102253198624, 0.023552825674414635, 0.02349190041422844, 0.023432355374097824, 0.02337583526968956, 0.023317769169807434, 0.02326449379324913, 0.023207830265164375, 0.023153385147452354, 0.02309618890285492, 0.023043304681777954, 0.022988472133874893, 0.0229327455163002, 0.02287965826690197, 0.02283012494444847, 0.022779691964387894, 0.022727277129888535, 0.02267567440867424, 0.02262832783162594, 0.022578375414013863, 0.022526655346155167, 0.022478466853499413, 0.022433536127209663, 0.02238810434937477, 0.02234194241464138]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data.iloc[idx, :-1].values\n",
    "        y = self.data.iloc[idx, -1]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def plot_loss_epoch(epoch,train_loss,test_loss,title):\n",
    "        \n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.plot(epoch,train_loss,color='b',label='train_loss')\n",
    "    plt.plot(epoch,test_loss,color='r',label='test_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.ylim(0,0.3)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_wine_quality(net:nn.Module,optimizer,epoches,lr,dataloader:Data.DataLoader,criterian):\n",
    "    \n",
    "    # save loss as epoch idx\n",
    "    epoch_idx = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        \n",
    "        # train loss\n",
    "        net.eval()\n",
    "        x = torch.tensor(train_df[:,:-1],dtype=torch.float32)\n",
    "        y = torch.tensor(train_df[:,-1],dtype=torch.float32).reshape(-1,1)\n",
    "        y_hat = net(x)\n",
    "        tr_loss = criterian(y,y_hat)\n",
    "        \n",
    "        \n",
    "        # train per epoch\n",
    "        for x,y in dataloader:\n",
    "            # calculate gradient\n",
    "            y = y.reshape(-1,1)\n",
    "            # print(\"x.shape\",x.shape)\n",
    "            # print(\"y.shape\",y.shape)\n",
    "            \n",
    "            net.train()\n",
    "            # forward\n",
    "            \n",
    "            # zero grad\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # forward \n",
    "            y_hat = net(x)\n",
    "            \n",
    "            # cal loss\n",
    "            loss = criterian(y,y_hat)\n",
    "\n",
    "            # back propagation\n",
    "            loss.sum().backward()\n",
    "            \n",
    "            # step\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        # test loss\n",
    "        net.eval()\n",
    "        x = torch.tensor(test_df[:,:-1],dtype=torch.float32)\n",
    "        y = torch.tensor(test_df[:,-1],dtype=torch.float32).reshape(-1,1)\n",
    "        y_hat = net(x)\n",
    "        te_loss = criterian(y,y_hat)\n",
    "        \n",
    "        # add to plot\n",
    "        epoch_idx.append(epoch)\n",
    "        test_loss.append(float(te_loss))\n",
    "        train_loss.append(float(tr_loss))\n",
    "        \n",
    "    # plot\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    plot_loss_epoch(epoch_idx,train_loss,test_loss,title=f'epoch={epoches} lr={lr}')\n",
    "\n",
    "\n",
    "epoches = 64 # 训练次数\n",
    "lr = 0.01 # 训练率\n",
    "batch_size = 256# 批量大小\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dataset_train = MyDataset('./wine_train.csv')\n",
    "train_loader = Data.DataLoader(dataset=dataset_train,batch_size=batch_size,\n",
    "                               shuffle=True,num_workers=0)\n",
    "\n",
    "train_wine_quality(net=net,optimizer=optimizer,epoches=epoches,\n",
    "                   lr=lr,dataloader=train_loader,criterian=criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaceea0",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第三部分:作业提交</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564e8a6",
   "metadata": {},
   "source": [
    "一、实验课下课前提交完成代码，如果下课前未完成，请将已经完成的部分进行提交，未完成的部分于之后的实验报告中进行补充  \n",
    "要求:  \n",
    "1)文件格式为：学号-姓名.ipynb  \n",
    "2)【不要】提交文件夹、压缩包、数据集等无关文件，只需提交单个ipynb文件即可"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af5b08",
   "metadata": {},
   "source": [
    "二、本次实验分为两周完成，实验报告提交截止日期: 12月15号14:20  \n",
    "要求：  \n",
    "1)文件格式为：学号-姓名.pdf  \n",
    "2)【不要】提交文件夹、压缩包、代码文件、数据集等任何与实验报告无关的文件，只需要提交单个pdf文件即可  \n",
    "3)文件命名时不需要额外添加“实验几”等额外信息，按照格式提交  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e73e3",
   "metadata": {},
   "source": [
    "实验十二(神经网络)的实验报告上交地址: https://send2me.cn/Wk9FsyYO/SKCBsWeFtvwQsg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db7ee4",
   "metadata": {},
   "source": [
    "三、课堂课件获取地址:https://www.jianguoyun.com/p/DWHYtsEQp5WhChjwtKoFIAA\n",
    "实验内容获取地址:https://www.jianguoyun.com/p/DbNY_SIQp5WhChjvtKoFIAA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
